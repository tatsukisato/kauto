{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d45a47f",
   "metadata": {
    "papermill": {
     "duration": 0.002406,
     "end_time": "2025-12-16T08:27:38.586006",
     "exception": false,
     "start_time": "2025-12-16T08:27:38.583600",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# exp006_phase2_embedding\n",
    "Embedding Head Verification (BN->FC->BN). Converted from script to notebook.\n",
    "- Keeps USE_ARCFACE flag (default False)\n",
    "- Same training/validation/inference flow as script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "309da015",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T08:27:38.591528Z",
     "iopub.status.busy": "2025-12-16T08:27:38.590773Z",
     "iopub.status.idle": "2025-12-16T08:27:50.328843Z",
     "shell.execute_reply": "2025-12-16T08:27:50.327981Z"
    },
    "papermill": {
     "duration": 11.742634,
     "end_time": "2025-12-16T08:27:50.330448",
     "exception": false,
     "start_time": "2025-12-16T08:27:38.587814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcc4b9de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T08:27:50.335291Z",
     "iopub.status.busy": "2025-12-16T08:27:50.334620Z",
     "iopub.status.idle": "2025-12-16T08:27:51.523950Z",
     "shell.execute_reply": "2025-12-16T08:27:51.523327Z"
    },
    "papermill": {
     "duration": 1.193218,
     "end_time": "2025-12-16T08:27:51.525344",
     "exception": false,
     "start_time": "2025-12-16T08:27:50.332126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '/kaggle/working/kauto'...\n"
     ]
    }
   ],
   "source": [
    "# Detect environment\n",
    "IS_KAGGLE = os.path.exists(\"/kaggle/input\")\n",
    "ROOT_DIR = Path(\"/kaggle/working/kauto/competitions\") if IS_KAGGLE else Path(__file__).resolve().parents[1]\n",
    "DATA_DIR = Path(\"/kaggle/input/atmacup22\") if IS_KAGGLE else Path(__file__).resolve().parents[1]\n",
    "\n",
    "# Clone repository if on Kaggle\n",
    "if IS_KAGGLE:\n",
    "    if not (Path(\"/kaggle/working/kauto\").exists()):\n",
    "        os.system(\"git clone https://github.com/tatsukisato/kauto.git /kaggle/working/kauto\")\n",
    "    sys.path.append(str(ROOT_DIR))\n",
    "else:\n",
    "    sys.path.append(str(ROOT_DIR))\n",
    "\n",
    "try:\n",
    "    from src.utils import (\n",
    "        setup_directories,\n",
    "        save_results,\n",
    "        create_submission,\n",
    "        print_experiment_info,\n",
    "        crop_and_save_images,\n",
    "    )\n",
    "    from src.dataset import AtmaCup22Dataset, MixedImageDataset\n",
    "    from src.models import AtmaCupModel\n",
    "    from src.generate_background import generate_background_samples\n",
    "    from src.image_dataset import ImageDataset as StandardImageDataset\n",
    "    from src.metrics import compute_evaluation_metrics\n",
    "except ImportError:\n",
    "    print(\"Warning: Custom modules not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "235fc573",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T08:27:51.530070Z",
     "iopub.status.busy": "2025-12-16T08:27:51.529855Z",
     "iopub.status.idle": "2025-12-16T08:47:23.119474Z",
     "shell.execute_reply": "2025-12-16T08:47:23.118337Z"
    },
    "papermill": {
     "duration": 1171.675837,
     "end_time": "2025-12-16T08:47:23.202896",
     "exception": false,
     "start_time": "2025-12-16T08:27:51.527059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Train data shape: (24920, 9)\n",
      "Test data shape: (9223, 9)\n",
      "Loading existing Background Samples...\n",
      "\n",
      "==================== Hold-out Validation (20%) ====================\n",
      "Using device: cuda, AMP: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 236MB/s]\n",
      "Ep1/12: 100%|██████████| 86/86 [01:36<00:00,  1.12s/it, loss=1.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val Loss: 1.5445\n",
      "  [Overall] F1: 0.4998\n",
      "  [Player ] F1: 0.4847\n",
      "  [BG Stats] Recall: 0.7341, Precision: 0.7356 (FP=133)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep2/12: 100%|██████████| 86/86 [01:23<00:00,  1.02it/s, loss=1.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val Loss: 1.4090\n",
      "  [Overall] F1: 0.5389\n",
      "  [Player ] F1: 0.5250\n",
      "  [BG Stats] Recall: 0.7540, Precision: 0.7646 (FP=117)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep3/12: 100%|██████████| 86/86 [01:24<00:00,  1.02it/s, loss=1.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val Loss: 1.4522\n",
      "  [Overall] F1: 0.5240\n",
      "  [Player ] F1: 0.5057\n",
      "  [BG Stats] Recall: 0.7817, Precision: 0.7864 (FP=107)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep4/12: 100%|██████████| 86/86 [01:26<00:00,  1.00s/it, loss=0.981]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val Loss: 1.4589\n",
      "  [Overall] F1: 0.5347\n",
      "  [Player ] F1: 0.5172\n",
      "  [BG Stats] Recall: 0.7837, Precision: 0.7948 (FP=102)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep5/12: 100%|██████████| 86/86 [01:27<00:00,  1.02s/it, loss=1.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val Loss: 1.3622\n",
      "  [Overall] F1: 0.5569\n",
      "  [Player ] F1: 0.5416\n",
      "  [BG Stats] Recall: 0.7956, Precision: 0.7786 (FP=114)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep6/12: 100%|██████████| 86/86 [01:26<00:00,  1.01s/it, loss=1.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val Loss: 1.3997\n",
      "  [Overall] F1: 0.5510\n",
      "  [Player ] F1: 0.5351\n",
      "  [BG Stats] Recall: 0.8115, Precision: 0.7574 (FP=131)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep7/12: 100%|██████████| 86/86 [01:26<00:00,  1.00s/it, loss=0.916]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val Loss: 1.4025\n",
      "  [Overall] F1: 0.5341\n",
      "  [Player ] F1: 0.5164\n",
      "  [BG Stats] Recall: 0.7639, Precision: 0.8297 (FP=79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep8/12: 100%|██████████| 86/86 [01:26<00:00,  1.00s/it, loss=1.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val Loss: 1.3769\n",
      "  [Overall] F1: 0.5505\n",
      "  [Player ] F1: 0.5354\n",
      "  [BG Stats] Recall: 0.8294, Precision: 0.7121 (FP=169)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep9/12: 100%|██████████| 86/86 [01:26<00:00,  1.01s/it, loss=0.983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val Loss: 1.2941\n",
      "  [Overall] F1: 0.5805\n",
      "  [Player ] F1: 0.5640\n",
      "  [BG Stats] Recall: 0.7877, Precision: 0.8706 (FP=59)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep10/12: 100%|██████████| 86/86 [01:28<00:00,  1.03s/it, loss=1.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val Loss: 1.2948\n",
      "  [Overall] F1: 0.5772\n",
      "  [Player ] F1: 0.5615\n",
      "  [BG Stats] Recall: 0.8274, Precision: 0.7809 (FP=117)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep11/12: 100%|██████████| 86/86 [01:27<00:00,  1.02s/it, loss=1.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val Loss: 1.3232\n",
      "  [Overall] F1: 0.5695\n",
      "  [Player ] F1: 0.5538\n",
      "  [BG Stats] Recall: 0.8254, Precision: 0.7689 (FP=125)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep12/12: 100%|██████████| 86/86 [01:24<00:00,  1.02it/s, loss=0.942]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val Loss: 1.3148\n",
      "  [Overall] F1: 0.5760\n",
      "  [Player ] F1: 0.5594\n",
      "  [BG Stats] Recall: 0.7956, Precision: 0.8496 (FP=71)\n",
      "Best Val F1 (Overall): 0.5805\n",
      "Submission saved to /kaggle/working/submissions/submission_exp006_phase2_embedding.csv\n",
      "Submission shape: (9223, 1)\n",
      "Label distribution:\n",
      "label_id\n",
      "-1     1318\n",
      " 0      193\n",
      " 1      654\n",
      " 2      627\n",
      " 3      726\n",
      " 4     1077\n",
      " 5      753\n",
      " 6      706\n",
      " 7      651\n",
      " 8      785\n",
      " 9     1025\n",
      " 10     708\n",
      "Name: count, dtype: int64\n",
      "Saved submission: /kaggle/working/submissions/submission_exp006_phase2_embedding.csv\n",
      "Results saved to /kaggle/working/output/exp006_phase2_embedding/exp006_phase2_embedding_results.json\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    exp_name = \"exp006_phase2_embedding\"\n",
    "    description = \"Phase 2 Step 3: Embedding Head Verification. use_embedding_head=True.\"\n",
    "    \n",
    "    DEBUG = not IS_KAGGLE\n",
    "    EPOCHS = 2 if DEBUG else 12\n",
    "    # ArcFace toggle: set True to enable ArcFace head/softmax-margin behavior.\n",
    "    # 注意: ArcFace 層は内部で F.normalize を行い、s=30.0, m=0.5 を想定しています。\n",
    "    # ArcFace に渡す入力は「L2 正規化 前 の embedding」を渡す（モデル内部で正規化される前提）。\n",
    "    USE_ARCFACE = False\n",
    "    \n",
    "    # Setup Directories\n",
    "    dirs = setup_directories(\n",
    "        base_dir=str(Path(\"/kaggle/working\") if IS_KAGGLE else ROOT_DIR),\n",
    "        data_dir=str(DATA_DIR)\n",
    "    )\n",
    "    print_experiment_info(exp_name, description)\n",
    "    \n",
    "    # 1. Load Data\n",
    "    raw_dir = dirs['raw']\n",
    "    dataset_handler = AtmaCup22Dataset(data_dir=str(raw_dir))\n",
    "    train_meta, test_meta = dataset_handler.load_data()\n",
    "    train_meta['original_index'] = train_meta.index\n",
    "\n",
    "    if DEBUG:\n",
    "        print(\"!!! DEBUG MODE: Using small subset !!!\")\n",
    "        unique_quarters = train_meta['quarter'].unique()\n",
    "        if len(unique_quarters) >= 2:\n",
    "            q1_df = train_meta[train_meta['quarter'] == unique_quarters[0]].head(100)\n",
    "            q2_df = train_meta[train_meta['quarter'] == unique_quarters[1]].head(100)\n",
    "            train_meta = pd.concat([q1_df, q2_df])\n",
    "        else:\n",
    "            train_meta = train_meta.iloc[:200]\n",
    "        test_meta = test_meta.iloc[:50]\n",
    "        \n",
    "    # 2. Check/Generate Player Crops\n",
    "    crops_dir = dirs['processed'] / 'crops_train'\n",
    "    if not list(crops_dir.glob(\"*.jpg\")):\n",
    "        print(f\"Generating player crops...\")\n",
    "        crop_and_save_images(train_meta, dirs['raw'], crops_dir, mode='train')\n",
    "    \n",
    "    # 3. Check/Generate Background Crops\n",
    "    bg_crops_dir = dirs['processed'] / 'crops_bg'\n",
    "    bg_csv_path = dirs['processed'] / 'train_meta_background.csv'\n",
    "    \n",
    "    if not bg_csv_path.exists():\n",
    "        print(\"Generating Background Samples...\")\n",
    "        bg_df = generate_background_samples(train_meta, dirs['raw'], bg_crops_dir, 1, -1)\n",
    "    else:\n",
    "        print(\"Loading existing Background Samples...\")\n",
    "        bg_df = pd.read_csv(bg_csv_path)\n",
    "    \n",
    "    bg_df['original_index'] = -1\n",
    "    full_train_df = pd.concat([train_meta, bg_df], axis=0, ignore_index=True)\n",
    "    \n",
    "    # 4. Transforms\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.6, 1.0), ratio=(0.8, 1.2)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # 5. Validation Split\n",
    "    sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    exp_output_dir = dirs['output'] / exp_name\n",
    "    exp_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model_dir = exp_output_dir / 'models'\n",
    "    model_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    groups = full_train_df['quarter']\n",
    "    X = full_train_df.index\n",
    "    y = full_train_df['label_id'].astype(str)\n",
    "    \n",
    "    train_idx, val_idx = next(sgkf.split(X, y, groups=groups))\n",
    "    \n",
    "    print(f\"\\n{'='*20} Hold-out Validation (20%) {'='*20}\")\n",
    "    train_df_fold = full_train_df.iloc[train_idx]\n",
    "    val_df_fold = full_train_df.iloc[val_idx]\n",
    "    \n",
    "    crop_dirs = {'train': crops_dir, 'bg': bg_crops_dir}\n",
    "    train_dataset = MixedImageDataset(train_df_fold, crop_dirs, transform=train_transform, mode='train')\n",
    "    val_dataset = MixedImageDataset(val_df_fold, crop_dirs, transform=val_transform, mode='validation')\n",
    "    \n",
    "    batch_size = 32 if DEBUG else 256\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "    )\n",
    "    \n",
    "    # 6. Model (12 Classes) - EMBEDDING HEAD ENABLED\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    use_amp = device.type == \"cuda\"\n",
    "    print(f\"Using device: {device}, AMP: {use_amp}\")\n",
    "    \n",
    "    model = AtmaCupModel(\n",
    "        num_classes=12,\n",
    "        pretrained=True,\n",
    "        freeze_backbone=True,\n",
    "        use_arcface=USE_ARCFACE,\n",
    "        use_embedding_head=True,\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # 学習率を大きすぎないように設定（1e-3 -> 3e-4 を推奨）\n",
    "    optimizer = optim.Adam(model.parameters(), lr=4e-3)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "    scaler = GradScaler(enabled=use_amp)\n",
    "    \n",
    "    best_score = 0.0\n",
    "    best_model_path = model_dir / f\"{exp_name}_best.pth\"\n",
    "    \n",
    "    # Training Loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"Ep{epoch+1}/{EPOCHS}\")\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                # If ArcFace is enabled, pass targets during forward so ArcFace layer\n",
    "                # can apply margin inside (model is expected to accept targets=labels).\n",
    "                # Important: pass embedding BEFORE external normalization; model's ArcFace\n",
    "                # layer will call F.normalize internally (s=30.0, m=0.5 assumed).\n",
    "                if USE_ARCFACE:\n",
    "                    outputs = model(images, targets=labels)\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        train_loss /= len(train_dataset)\n",
    "        \n",
    "        # Val\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                with autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                    # In eval, do not pass targets so model returns inference scores\n",
    "                    if USE_ARCFACE:\n",
    "                        outputs = model(images)  # returns scaled cosine (inference)\n",
    "                    else:\n",
    "                        outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                \n",
    "                preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "                val_preds.extend(preds)\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_loss /= len(val_dataset)\n",
    "        \n",
    "        # Validation Metrics (delegated to src.metrics)\n",
    "        val_labels = np.array(val_labels)\n",
    "        val_preds = np.array(val_preds)\n",
    "        metrics = compute_evaluation_metrics(val_labels, val_preds, bg_label=11)\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"  [Overall] F1: {metrics['macro_f1_all']:.4f}\")\n",
    "        print(f\"  [Player ] F1: {metrics['macro_f1_player']:.4f}\")\n",
    "        print(f\"  [BG Stats] Recall: {metrics['bg_recall']:.4f}, Precision: {metrics['bg_precision']:.4f} (FP={metrics['bg_fp']})\")\n",
    "        macro_f1_all = metrics[\"macro_f1_all\"]\n",
    "        macro_f1_player = metrics[\"macro_f1_player\"]\n",
    "\n",
    "        if macro_f1_all > best_score:\n",
    "            best_score = macro_f1_all\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            \n",
    "        scheduler.step(macro_f1_all)\n",
    "        \n",
    "    print(f\"Best Val F1 (Overall): {best_score:.4f}\")\n",
    "    # --- Inference on Test and create submission ---\n",
    "    try:\n",
    "        if best_model_path.exists():\n",
    "            model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "            model.eval()\n",
    "            test_dataset = StandardImageDataset(test_meta, str(dirs['raw']), transform=val_transform, mode='test')\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "            final_test_preds = []\n",
    "            with torch.no_grad():\n",
    "                for images in test_loader:\n",
    "                    images = images.to(device)\n",
    "                    with autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                        outputs = model(images)\n",
    "                    preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "                    preds = np.where(preds == 11, -1, preds)  # map BG class back to -1\n",
    "                    final_test_preds.extend(preds)\n",
    "\n",
    "            sub_path = dirs['submissions'] / f\"submission_{exp_name}.csv\"\n",
    "            create_submission(final_test_preds, str(sub_path), test_meta)\n",
    "            print(f\"Saved submission: {sub_path}\")\n",
    "        else:\n",
    "            print(\"No best model found for inference; skipping test inference.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Test inference failed: {e}\")\n",
    "\n",
    "    save_results({'val_score_overall': best_score, 'val_score_player': macro_f1_player}, str(exp_output_dir), exp_name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1925b02d",
   "metadata": {
    "papermill": {
     "duration": 0.082944,
     "end_time": "2025-12-16T08:47:23.370404",
     "exception": false,
     "start_time": "2025-12-16T08:47:23.287460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 9021091,
     "sourceId": 14165212,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1191.119509,
   "end_time": "2025-12-16T08:47:26.043720",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-16T08:27:34.924211",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
