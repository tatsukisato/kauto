{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "050571ea",
   "metadata": {
    "papermill": {
     "duration": 0.003354,
     "end_time": "2025-12-16T09:28:30.155833",
     "exception": false,
     "start_time": "2025-12-16T09:28:30.152479",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# exp006_phase2_arcface_embedding\n",
    "Exp006: ArcFace + Embedding Head の実験ノートブック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2125cc43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T09:28:30.165394Z",
     "iopub.status.busy": "2025-12-16T09:28:30.164746Z",
     "iopub.status.idle": "2025-12-16T09:28:43.598169Z",
     "shell.execute_reply": "2025-12-16T09:28:43.597442Z"
    },
    "papermill": {
     "duration": 13.439348,
     "end_time": "2025-12-16T09:28:43.599783",
     "exception": false,
     "start_time": "2025-12-16T09:28:30.160435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '/kaggle/working/kauto'...\n"
     ]
    }
   ],
   "source": [
    "# === Added for exp006 improvement ===\n",
    "# Exp006: Improvements Phase 2 Step 2 (ArcFace Minimal Intro)\n",
    "# Base: Exp006 Phase 2 Step 1\n",
    "# Improvements:\n",
    "# 1. Enable use_arcface=True in AtmaCupModel\n",
    "# 2. Verify that training runs and loss decreases\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from typing import Sized, cast\n",
    "\n",
    "# Detect environment\n",
    "IS_KAGGLE = os.path.exists(\"/kaggle/input\")\n",
    "ROOT_DIR = Path(\"/kaggle/working/kauto/competitions\") if IS_KAGGLE else Path(__file__).resolve().parents[1]\n",
    "DATA_DIR = Path(\"/kaggle/input/atmacup22\") if IS_KAGGLE else Path(__file__).resolve().parents[1]\n",
    "\n",
    "# Clone repository if on Kaggle\n",
    "if IS_KAGGLE:\n",
    "    if not (Path(\"/kaggle/working/kauto\").exists()):\n",
    "        os.system(\"git clone https://github.com/tatsukisato/kauto.git /kaggle/working/kauto\")\n",
    "    sys.path.append(str(ROOT_DIR))\n",
    "else:\n",
    "    sys.path.append(str(ROOT_DIR))\n",
    "\n",
    "try:\n",
    "    from src.utils import (\n",
    "        setup_directories,\n",
    "        save_results,\n",
    "        create_submission,\n",
    "        print_experiment_info,\n",
    "        crop_and_save_images,\n",
    "    )\n",
    "    from src.dataset import AtmaCup22Dataset, MixedImageDataset\n",
    "    from src.models import AtmaCupModel\n",
    "    from src.generate_background import generate_background_samples\n",
    "    from src.image_dataset import ImageDataset as StandardImageDataset\n",
    "    from src.metrics import compute_evaluation_metrics\n",
    "except ImportError:\n",
    "    print(\"Warning: Custom modules not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "559940a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T09:28:43.605800Z",
     "iopub.status.busy": "2025-12-16T09:28:43.605342Z",
     "iopub.status.idle": "2025-12-16T09:28:43.615432Z",
     "shell.execute_reply": "2025-12-16T09:28:43.614728Z"
    },
    "papermill": {
     "duration": 0.014602,
     "end_time": "2025-12-16T09:28:43.616690",
     "exception": false,
     "start_time": "2025-12-16T09:28:43.602088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (このファイルは単体実行用。既存 experiments のパターンに合わせています。)\n",
    "\n",
    "def build_transforms():\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.6, 1.0), ratio=(0.8, 1.2)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return train_transform, val_transform\n",
    "\n",
    "def load_and_prepare_data(dirs, debug):\n",
    "    ds = AtmaCup22Dataset(data_dir=str(dirs['raw']))\n",
    "    train_meta, test_meta = ds.load_data()\n",
    "    train_meta['original_index'] = train_meta.index\n",
    "\n",
    "    if debug:\n",
    "        uq = train_meta['quarter'].unique()\n",
    "        if len(uq) >= 2:\n",
    "            train_meta = pd.concat([\n",
    "                train_meta[train_meta['quarter'] == uq[0]].head(100),\n",
    "                train_meta[train_meta['quarter'] == uq[1]].head(100),\n",
    "            ])\n",
    "        else:\n",
    "            train_meta = train_meta.head(200)\n",
    "        test_meta = test_meta.head(50)\n",
    "\n",
    "    crops_dir = dirs['processed'] / 'crops_train'\n",
    "    if not list(crops_dir.glob(\"*.jpg\")):\n",
    "        print(\"Generating player crops...\")\n",
    "        crop_and_save_images(train_meta, dirs['raw'], crops_dir, mode='train')\n",
    "\n",
    "    bg_crops_dir = dirs['processed'] / 'crops_bg'\n",
    "    bg_csv_path = dirs['processed'] / 'train_meta_background.csv'\n",
    "    if not bg_csv_path.exists():\n",
    "        print(\"Generating Background Samples...\")\n",
    "        bg_df = generate_background_samples(train_meta, dirs['raw'], bg_crops_dir, 1, -1)\n",
    "    else:\n",
    "        print(\"Loading existing Background Samples...\")\n",
    "        bg_df = pd.read_csv(bg_csv_path)\n",
    "\n",
    "    bg_df['original_index'] = -1\n",
    "    full_train_df = pd.concat([train_meta, bg_df], axis=0, ignore_index=True)\n",
    "    # 変更: train_meta を戻り値に追加\n",
    "    return full_train_df, train_meta, test_meta, crops_dir, bg_crops_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60c69aa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T09:28:43.622551Z",
     "iopub.status.busy": "2025-12-16T09:28:43.621858Z",
     "iopub.status.idle": "2025-12-16T09:28:43.628618Z",
     "shell.execute_reply": "2025-12-16T09:28:43.628031Z"
    },
    "papermill": {
     "duration": 0.010973,
     "end_time": "2025-12-16T09:28:43.629931",
     "exception": false,
     "start_time": "2025-12-16T09:28:43.618958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_dataloaders(full_train_df, crops_dir, bg_crops_dir, train_transform, val_transform, debug, batch_size=256, num_workers=4):\n",
    "    sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    X = full_train_df.index.to_numpy()\n",
    "    y = full_train_df['label_id'].astype(str).to_numpy()\n",
    "    groups = full_train_df['quarter'].to_numpy()\n",
    "    train_idx, val_idx = next(sgkf.split(X, y, groups=groups))\n",
    "    train_df_fold = full_train_df.iloc[train_idx]\n",
    "    val_df_fold = full_train_df.iloc[val_idx]\n",
    "\n",
    "    crop_dirs = {'train': crops_dir, 'bg': bg_crops_dir}\n",
    "    train_dataset = MixedImageDataset(train_df_fold, crop_dirs, transform=train_transform, mode='train')\n",
    "    val_dataset = MixedImageDataset(val_df_fold, crop_dirs, transform=val_transform, mode='validation')\n",
    "\n",
    "    bsize = 32 if debug else batch_size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=bsize, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=bsize, shuffle=False, num_workers=num_workers)\n",
    "    return train_loader, val_loader, train_dataset, val_dataset\n",
    "\n",
    "def build_model(device, use_arcface=True, use_embedding_head=True):\n",
    "    model = AtmaCupModel(\n",
    "        num_classes=12,\n",
    "        pretrained=True,\n",
    "        freeze_backbone=True,\n",
    "        use_arcface=use_arcface,\n",
    "        use_embedding_head=use_embedding_head,\n",
    "    )\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b79da416",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T09:28:43.635295Z",
     "iopub.status.busy": "2025-12-16T09:28:43.635065Z",
     "iopub.status.idle": "2025-12-16T09:28:43.644831Z",
     "shell.execute_reply": "2025-12-16T09:28:43.644235Z"
    },
    "papermill": {
     "duration": 0.013943,
     "end_time": "2025-12-16T09:28:43.646065",
     "exception": false,
     "start_time": "2025-12-16T09:28:43.632122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate_and_metrics(model, val_loader, device, use_amp, criterion):\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            with autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            val_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    metrics = compute_evaluation_metrics(val_labels, val_preds, bg_label=11)\n",
    "    return val_loss, metrics\n",
    "\n",
    "def train_loop(model, train_loader, val_loader, device, use_amp, criterion, optimizer, scheduler, scaler, exp_output_dir, best_model_path, epochs=12):\n",
    "    best_score = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"Ep{epoch+1}/{epochs}\")\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                # For ArcFace training, model(images, targets=labels) handled by model impl if needed\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        val_loss, metrics = validate_and_metrics(model, val_loader, device, use_amp, criterion)\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  [Val] Loss: {val_loss:.4f} | F1: {metrics['macro_f1_all']:.4f} | Player: {metrics['macro_f1_player']:.4f} | BG prec: {metrics['bg_precision']:.4f}\")\n",
    "\n",
    "        if metrics['macro_f1_all'] > best_score:\n",
    "            best_score = metrics['macro_f1_all']\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "        scheduler.step(metrics['macro_f1_all'])\n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0572582",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T09:28:43.651628Z",
     "iopub.status.busy": "2025-12-16T09:28:43.651139Z",
     "iopub.status.idle": "2025-12-16T09:28:43.657667Z",
     "shell.execute_reply": "2025-12-16T09:28:43.657011Z"
    },
    "papermill": {
     "duration": 0.010699,
     "end_time": "2025-12-16T09:28:43.658933",
     "exception": false,
     "start_time": "2025-12-16T09:28:43.648234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def infer_and_submit(model, best_model_path, test_meta, dirs, val_transform, batch_size, num_workers, device, use_amp, exp_name):\n",
    "    try:\n",
    "        if best_model_path.exists():\n",
    "            model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "            model.eval()\n",
    "            test_dataset = StandardImageDataset(test_meta, str(dirs['raw']), transform=val_transform, mode='test')\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "            final_test_preds = []\n",
    "            with torch.no_grad():\n",
    "                for images in test_loader:\n",
    "                    images = images.to(device)\n",
    "                    with autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                        outputs = model(images)\n",
    "                    preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "                    preds = np.where(preds == 11, -1, preds)\n",
    "                    final_test_preds.extend(preds)\n",
    "            sub_path = dirs['submissions'] / f\"submission_{exp_name}.csv\"\n",
    "            create_submission(final_test_preds, str(sub_path), test_meta)\n",
    "            print(f\"Saved submission: {sub_path}\")\n",
    "        else:\n",
    "            print(\"No best model found for inference; skipping test inference.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Test inference failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "851a0c32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T09:28:43.664713Z",
     "iopub.status.busy": "2025-12-16T09:28:43.664449Z",
     "iopub.status.idle": "2025-12-16T09:54:06.781626Z",
     "shell.execute_reply": "2025-12-16T09:54:06.780559Z"
    },
    "papermill": {
     "duration": 1523.214846,
     "end_time": "2025-12-16T09:54:06.876018",
     "exception": false,
     "start_time": "2025-12-16T09:28:43.661172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Train data shape: (24920, 9)\n",
      "Test data shape: (9223, 9)\n",
      "Loading existing Background Samples...\n",
      "Loading existing Background Samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 163MB/s]\n",
      "Ep1/12: 100%|██████████| 86/86 [01:52<00:00,  1.31s/it, loss=1.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 1.4442\n",
      "  [Val] Loss: 1.4661 | F1: 0.5248 | Player: 0.5085 | BG prec: 0.8176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep2/12: 100%|██████████| 86/86 [01:39<00:00,  1.16s/it, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 1.1347\n",
      "  [Val] Loss: 1.4150 | F1: 0.5336 | Player: 0.5189 | BG prec: 0.8160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep3/12: 100%|██████████| 86/86 [01:39<00:00,  1.15s/it, loss=1.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 1.1046\n",
      "  [Val] Loss: 1.4069 | F1: 0.5329 | Player: 0.5192 | BG prec: 0.7099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep4/12: 100%|██████████| 86/86 [01:55<00:00,  1.34s/it, loss=1.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 1.0817\n",
      "  [Val] Loss: 1.3815 | F1: 0.5473 | Player: 0.5346 | BG prec: 0.7308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep5/12: 100%|██████████| 86/86 [02:07<00:00,  1.49s/it, loss=1.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 1.0742\n",
      "  [Val] Loss: 1.3829 | F1: 0.5483 | Player: 0.5344 | BG prec: 0.6970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep6/12: 100%|██████████| 86/86 [02:03<00:00,  1.43s/it, loss=0.993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 1.0475\n",
      "  [Val] Loss: 1.3505 | F1: 0.5550 | Player: 0.5408 | BG prec: 0.7567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep7/12: 100%|██████████| 86/86 [01:57<00:00,  1.36s/it, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 1.0551\n",
      "  [Val] Loss: 1.3372 | F1: 0.5603 | Player: 0.5435 | BG prec: 0.8840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep8/12: 100%|██████████| 86/86 [01:48<00:00,  1.27s/it, loss=1.19]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 1.0469\n",
      "  [Val] Loss: 1.3074 | F1: 0.5651 | Player: 0.5482 | BG prec: 0.8776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep9/12: 100%|██████████| 86/86 [01:46<00:00,  1.23s/it, loss=0.917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 1.0384\n",
      "  [Val] Loss: 1.3410 | F1: 0.5582 | Player: 0.5398 | BG prec: 0.8864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep10/12: 100%|██████████| 86/86 [01:48<00:00,  1.27s/it, loss=1.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 1.0314\n",
      "  [Val] Loss: 1.3182 | F1: 0.5647 | Player: 0.5460 | BG prec: 0.8323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep11/12: 100%|██████████| 86/86 [02:08<00:00,  1.49s/it, loss=1.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 1.0278\n",
      "  [Val] Loss: 1.2722 | F1: 0.5832 | Player: 0.5683 | BG prec: 0.8059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep12/12: 100%|██████████| 86/86 [01:44<00:00,  1.22s/it, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 1.0383\n",
      "  [Val] Loss: 1.3236 | F1: 0.5718 | Player: 0.5551 | BG prec: 0.8767\n",
      "Submission saved to /kaggle/working/submissions/submission_exp006_phase2_arcface_embedding.csv\n",
      "Submission shape: (9223, 1)\n",
      "Label distribution:\n",
      "label_id\n",
      "-1     1595\n",
      " 0      184\n",
      " 1      566\n",
      " 2      616\n",
      " 3      873\n",
      " 4     1092\n",
      " 5      633\n",
      " 6      535\n",
      " 7      670\n",
      " 8      902\n",
      " 9      849\n",
      " 10     708\n",
      "Name: count, dtype: int64\n",
      "Saved submission: /kaggle/working/submissions/submission_exp006_phase2_arcface_embedding.csv\n",
      "Results saved to /kaggle/working/output/exp006_phase2_arcface_embedding/exp006_phase2_arcface_embedding_results.json\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 1) 実験名・説明・デバッグ設定の初期化\n",
    "    exp_name = \"exp006_phase2_arcface_embedding\"\n",
    "    description = \"Phase2 D: ArcFace + Embedding Head (no EMA)\"\n",
    "    DEBUG = not IS_KAGGLE\n",
    "    EPOCHS = 2 if DEBUG else 12\n",
    "\n",
    "    # 2) パス/ディレクトリのセットアップ（Kaggle/ローカル共通）\n",
    "    dirs = setup_directories(base_dir=str(Path(\"/kaggle/working\") if IS_KAGGLE else ROOT_DIR), data_dir=str(DATA_DIR))\n",
    "    print_experiment_info(exp_name, description)\n",
    "\n",
    "    # 3) データ読み込みと（DEBUG時の）サブサンプリング\n",
    "    #    - train_meta, test_meta を読み込み original_index を付与\n",
    "    #    - DEBUG モードなら少量データに切り替え\n",
    "    # 変更: train_meta を受け取るようにアンパック\n",
    "    full_train_df, train_meta, test_meta, crops_dir, bg_crops_dir = load_and_prepare_data(dirs, DEBUG)\n",
    "\n",
    "    # 4) プレイヤー crop と背景サンプルの確認・生成\n",
    "    #    - crops_dir に画像がなければ crop を生成\n",
    "    #    - 背景 CSV がなければ背景サンプルを生成\n",
    "    crops_dir = dirs['processed'] / 'crops_train'\n",
    "    if not list(crops_dir.glob(\"*.jpg\")):\n",
    "        print(\"Generating player crops...\")\n",
    "        crop_and_save_images(train_meta, dirs['raw'], crops_dir, mode='train')\n",
    "\n",
    "    bg_crops_dir = dirs['processed'] / 'crops_bg'\n",
    "    bg_csv_path = dirs['processed'] / 'train_meta_background.csv'\n",
    "    if not bg_csv_path.exists():\n",
    "        print(\"Generating Background Samples...\")\n",
    "        bg_df = generate_background_samples(train_meta, dirs['raw'], bg_crops_dir, 1, -1)\n",
    "    else:\n",
    "        print(\"Loading existing Background Samples...\")\n",
    "        bg_df = pd.read_csv(bg_csv_path)\n",
    "\n",
    "    bg_df['original_index'] = -1\n",
    "    full_train_df = pd.concat([train_meta, bg_df], axis=0, ignore_index=True)\n",
    "\n",
    "    # 5) 画像変換（train/val）設定\n",
    "    train_transform, val_transform = build_transforms()\n",
    "\n",
    "    # 6) StratifiedGroupKFold によるホールドアウト分割（group=quarter）\n",
    "    #    - train/val DataFrame を作成\n",
    "    sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    X = full_train_df.index.to_numpy()\n",
    "    y = full_train_df['label_id'].astype(str).to_numpy()\n",
    "    groups = full_train_df['quarter'].to_numpy()\n",
    "    train_idx, val_idx = next(sgkf.split(X, y, groups=groups))\n",
    "    train_df_fold = full_train_df.iloc[train_idx]\n",
    "    val_df_fold = full_train_df.iloc[val_idx]\n",
    "\n",
    "    # 7) Dataset / DataLoader の作成\n",
    "    #    - MixedImageDataset を使って train/val loader を生成\n",
    "    #    - batch_size, num_workers の設定\n",
    "    crop_dirs = {'train': crops_dir, 'bg': bg_crops_dir}\n",
    "    train_dataset = MixedImageDataset(train_df_fold, crop_dirs, transform=train_transform, mode='train')\n",
    "    val_dataset = MixedImageDataset(val_df_fold, crop_dirs, transform=val_transform, mode='validation')\n",
    "\n",
    "    bsize = 32 if DEBUG else 256\n",
    "    train_loader = DataLoader(train_dataset, batch_size=bsize, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=bsize, shuffle=False, num_workers=4)\n",
    "\n",
    "    # 8) モデル / 損失 / 最適化器 / スケジューラ / AMP スケーラ の構築\n",
    "    #    - AtmaCupModel を ArcFace + Embedding の組合せで構築\n",
    "    #    - optimizer.lr は安定化のため小さめに設定\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    use_amp = device.type == \"cuda\"\n",
    "    model = AtmaCupModel(\n",
    "        num_classes=12,\n",
    "        pretrained=True,\n",
    "        freeze_backbone=True,\n",
    "        use_arcface=True,\n",
    "        use_embedding_head=True,\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "    scaler = GradScaler(enabled=use_amp)\n",
    "\n",
    "    # 9) 学習ループ（エポック毎）\n",
    "    #    - train モードでバッチ学習（AMP あり）\n",
    "    #    - バックプロパゲーションと optimizer.step(), scaler.update()\n",
    "    #    - バリデーション実行 -> compute_evaluation_metrics を使用して指標算出\n",
    "    #    - ベストモデル保存と scheduler.step() の更新\n",
    "    best_score = 0.0\n",
    "    exp_output_dir = dirs['output'] / exp_name\n",
    "    exp_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model_dir = exp_output_dir / 'models'\n",
    "    model_dir.mkdir(exist_ok=True)\n",
    "    best_model_path = model_dir / f\"{exp_name}_best.pth\"\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"Ep{epoch+1}/{EPOCHS}\")\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                # For ArcFace training, model(images, targets=labels) handled by model impl if needed\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "        train_loss /= len(cast(Sized, train_loader.dataset))\n",
    "\n",
    "        val_loss, metrics = validate_and_metrics(model, val_loader, device, use_amp, criterion)\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  [Val] Loss: {val_loss:.4f} | F1: {metrics['macro_f1_all']:.4f} | Player: {metrics['macro_f1_player']:.4f} | BG prec: {metrics['bg_precision']:.4f}\")\n",
    "\n",
    "        if metrics['macro_f1_all'] > best_score:\n",
    "            best_score = metrics['macro_f1_all']\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "        scheduler.step(metrics['macro_f1_all'])\n",
    "\n",
    "    # 10) テスト推論と submission 作成\n",
    "    #     - ベストモデルを読み込み、test 用 Dataset で推論\n",
    "    #     - BG クラス(11) を -1 に戻して create_submission を呼ぶ\n",
    "    try:\n",
    "        if best_model_path.exists():\n",
    "            model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "            model.eval()\n",
    "            test_dataset = StandardImageDataset(test_meta, str(dirs['raw']), transform=val_transform, mode='test')\n",
    "            test_loader = DataLoader(test_dataset, batch_size=(32 if DEBUG else 256), shuffle=False, num_workers=4)\n",
    "            final_test_preds = []\n",
    "            with torch.no_grad():\n",
    "                for images in test_loader:\n",
    "                    images = images.to(device)\n",
    "                    with autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                        outputs = model(images)\n",
    "                    preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "                    preds = np.where(preds == 11, -1, preds)\n",
    "                    final_test_preds.extend(preds)\n",
    "            sub_path = dirs['submissions'] / f\"submission_{exp_name}.csv\"\n",
    "            create_submission(final_test_preds, str(sub_path), test_meta)\n",
    "            print(f\"Saved submission: {sub_path}\")\n",
    "        else:\n",
    "            print(\"No best model found for inference; skipping test inference.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Test inference failed: {e}\")\n",
    "\n",
    "    # 11) 結果保存（save_results）\n",
    "    save_results({'val_score_overall': best_score}, str(exp_output_dir), exp_name)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 9021091,
     "sourceId": 14165212,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1543.538558,
   "end_time": "2025-12-16T09:54:09.663773",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-16T09:28:26.125215",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
